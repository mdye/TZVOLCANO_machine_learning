{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toward autonomous detection of anomalous GNSS data via applied unsupervised artificial intelligence \n",
    "\n",
    "Unsupervised Anomaly Detection of TZVOLCANO GNSS Data using Gaussian Mixtures. Once loaded in Binder, please run all the cells to properly initialize values and GUI elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "- Author1 = {\"name\": \"Mike Dye\", \"affiliation\": \"Unaffiliated\", \"email\": \"mike@mikedye.com\", \"orcid\": \"0000-0003-2065-870X\"}\n",
    "- Author2 = {\"name\": \"Dr. Sarah Stamps\", \"affiliation\": \"Virginia Tech\", \"email\": \"dstamps@vt.edu\", \"orcid\": \"0000-0002-3531-1752\"}\n",
    "- Author3 = {\"name\": \"Myles Mason\", \"affiliation\": \"Virginia Tech\", \"email\": \"mylesm18@vt.edu\", \"orcid\": \"\"}\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Toward-autonomous-detection-of-anomalous-GNSS-data-via-applied-unsupervised-artificial-intelligence\" data-toc-modified-id=\"Toward-autonomous-detection-of-anomalous-GNSS-data-via-applied-unsupervised-artificial-intelligence-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Toward autonomous detection of anomalous GNSS data via applied unsupervised artificial intelligence</a></span><ul class=\"toc-item\"><li><span><a href=\"#Authors\" data-toc-modified-id=\"Authors-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Authors</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Technical-contributions\" data-toc-modified-id=\"Technical-contributions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Technical contributions</a></span></li><li><span><a href=\"#Methodology\" data-toc-modified-id=\"Methodology-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Methodology</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Results</a></span></li><li><span><a href=\"#Funding\" data-toc-modified-id=\"Funding-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Funding</a></span></li><li><span><a href=\"#Keywords\" data-toc-modified-id=\"Keywords-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Keywords</a></span></li><li><span><a href=\"#Citation\" data-toc-modified-id=\"Citation-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Citation</a></span></li><li><span><a href=\"#Work-In-Progress---improvements\" data-toc-modified-id=\"Work-In-Progress---improvements-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Work In Progress - improvements</a></span></li><li><span><a href=\"#Suggested-next-steps\" data-toc-modified-id=\"Suggested-next-steps-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Suggested next steps</a></span></li><li><span><a href=\"#Acknowledgements\" data-toc-modified-id=\"Acknowledgements-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Acknowledgements</a></span></li><li><span><a href=\"#License\" data-toc-modified-id=\"License-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>License</a></span></li><li><span><a href=\"#Glossary\" data-toc-modified-id=\"Glossary-1.13\"><span class=\"toc-item-num\">1.13&nbsp;&nbsp;</span>Glossary</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Library-import\" data-toc-modified-id=\"Library-import-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Library import</a></span></li><li><span><a href=\"#Local-library-import\" data-toc-modified-id=\"Local-library-import-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Local library import</a></span></li></ul></li><li><span><a href=\"#Parameter-definitions\" data-toc-modified-id=\"Parameter-definitions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Parameter definitions</a></span></li><li><span><a href=\"#Data-import\" data-toc-modified-id=\"Data-import-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data import</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Important-note-on-imported-data\" data-toc-modified-id=\"Important-note-on-imported-data-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Important note on imported data</a></span></li></ul></li><li><span><a href=\"#Select-the-instrument-ID-and-the-start-and-end-date-of-the-data-to-be-processed-and-analyzed\" data-toc-modified-id=\"Select-the-instrument-ID-and-the-start-and-end-date-of-the-data-to-be-processed-and-analyzed-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Select the instrument ID and the start and end date of the data to be processed and analyzed</a></span></li><li><span><a href=\"#Select-data-file-for-use-during-analysis\" data-toc-modified-id=\"Select-data-file-for-use-during-analysis-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Select data file for use during analysis</a></span></li><li><span><a href=\"#Read-contents-of-the-selected-file-in-to-a-pandas-object\" data-toc-modified-id=\"Read-contents-of-the-selected-file-in-to-a-pandas-object-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Read contents of the selected file in to a pandas object</a></span></li></ul></li><li><span><a href=\"#Data-processing-and-analysis\" data-toc-modified-id=\"Data-processing-and-analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data processing and analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Resample-the-data-to-fill-in-holes-in-the-time-series\" data-toc-modified-id=\"Resample-the-data-to-fill-in-holes-in-the-time-series-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Resample the data to fill in holes in the time series</a></span><ul class=\"toc-item\"><li><span><a href=\"#Make-a-copy-of-the-unmodified-data\" data-toc-modified-id=\"Make-a-copy-of-the-unmodified-data-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Make a copy of the unmodified data</a></span></li><li><span><a href=\"#Alternative-method:-Make-copy-of-unmodified-data-and-resample-the-time-series-to-fill-in-any-missing-time-values-in-the-series\" data-toc-modified-id=\"Alternative-method:-Make-copy-of-unmodified-data-and-resample-the-time-series-to-fill-in-any-missing-time-values-in-the-series-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Alternative method: Make copy of unmodified data <strong>and</strong> resample the time series to fill in any missing time values in the series</a></span></li></ul></li><li><span><a href=\"#Rescale-the-Variables\" data-toc-modified-id=\"Rescale-the-Variables-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Rescale the Variables</a></span></li><li><span><a href=\"#Calculate-the-Vector-Magnitude\" data-toc-modified-id=\"Calculate-the-Vector-Magnitude-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Calculate the Vector Magnitude</a></span></li><li><span><a href=\"#Plot-the-Variables\" data-toc-modified-id=\"Plot-the-Variables-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Plot the Variables</a></span></li><li><span><a href=\"#Identify-outliers-for-each-variable-with-the-Gaussian-Mixtures-algorithm\" data-toc-modified-id=\"Identify-outliers-for-each-variable-with-the-Gaussian-Mixtures-algorithm-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Identify outliers for each variable with the Gaussian Mixtures algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-outliers-for-the-Scaled-Height-variable\" data-toc-modified-id=\"Find-outliers-for-the-Scaled-Height-variable-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>Find outliers for the Scaled Height variable</a></span></li><li><span><a href=\"#Find-outliers-for-the-Scaled-Latitude-variable\" data-toc-modified-id=\"Find-outliers-for-the-Scaled-Latitude-variable-5.5.2\"><span class=\"toc-item-num\">5.5.2&nbsp;&nbsp;</span>Find outliers for the Scaled Latitude variable</a></span></li><li><span><a href=\"#Find-outliers-for-the-Scaled-Longitude-variable\" data-toc-modified-id=\"Find-outliers-for-the-Scaled-Longitude-variable-5.5.3\"><span class=\"toc-item-num\">5.5.3&nbsp;&nbsp;</span>Find outliers for the Scaled Longitude variable</a></span></li><li><span><a href=\"#Find-outliers-for-the-Vector-Magnitude-variable\" data-toc-modified-id=\"Find-outliers-for-the-Vector-Magnitude-variable-5.5.4\"><span class=\"toc-item-num\">5.5.4&nbsp;&nbsp;</span>Find outliers for the Vector Magnitude variable</a></span></li><li><span><a href=\"#Consolidate-the-arrays-of-times-to-remove-in-to-one-object\" data-toc-modified-id=\"Consolidate-the-arrays-of-times-to-remove-in-to-one-object-5.5.5\"><span class=\"toc-item-num\">5.5.5&nbsp;&nbsp;</span>Consolidate the arrays of times to remove in to one object</a></span></li><li><span><a href=\"#Create-a-new-pandas-object-with-the-points-identified-by-the-algorith-removed\" data-toc-modified-id=\"Create-a-new-pandas-object-with-the-points-identified-by-the-algorith-removed-5.5.6\"><span class=\"toc-item-num\">5.5.6&nbsp;&nbsp;</span>Create a new pandas object with the points identified by the algorith removed</a></span></li><li><span><a href=\"#Plot-the-&quot;cleansed&quot;-data-on-top-of-the-orginal-data\" data-toc-modified-id=\"Plot-the-&quot;cleansed&quot;-data-on-top-of-the-orginal-data-5.5.7\"><span class=\"toc-item-num\">5.5.7&nbsp;&nbsp;</span>Plot the \"cleansed\" data on top of the orginal data</a></span></li></ul></li><li><span><a href=\"#Identify-anomalies-for-each-variable-using-the-K-means-algorithm\" data-toc-modified-id=\"Identify-anomalies-for-each-variable-using-the-K-means-algorithm-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Identify anomalies for each variable using the K-means algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#find-outliers-for-the-Scaled-Height-variable\" data-toc-modified-id=\"find-outliers-for-the-Scaled-Height-variable-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>find outliers for the Scaled Height variable</a></span></li><li><span><a href=\"#find-outliers-for-the-Scaled-Latitude-variable\" data-toc-modified-id=\"find-outliers-for-the-Scaled-Latitude-variable-5.6.2\"><span class=\"toc-item-num\">5.6.2&nbsp;&nbsp;</span>find outliers for the Scaled Latitude variable</a></span></li><li><span><a href=\"#find-outliers-for-the-Scaled-Longitude-variable\" data-toc-modified-id=\"find-outliers-for-the-Scaled-Longitude-variable-5.6.3\"><span class=\"toc-item-num\">5.6.3&nbsp;&nbsp;</span>find outliers for the Scaled Longitude variable</a></span></li><li><span><a href=\"#find-outliers-for-the-Vector-Magnitude-variable\" data-toc-modified-id=\"find-outliers-for-the-Vector-Magnitude-variable-5.6.4\"><span class=\"toc-item-num\">5.6.4&nbsp;&nbsp;</span>find outliers for the Vector Magnitude variable</a></span></li></ul></li><li><span><a href=\"#Training-the-neural-net\" data-toc-modified-id=\"Training-the-neural-net-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Training the neural net</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-on-Unfiltered-data\" data-toc-modified-id=\"Training-on-Unfiltered-data-5.7.1\"><span class=\"toc-item-num\">5.7.1&nbsp;&nbsp;</span>Training on Unfiltered data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-the-training-set\" data-toc-modified-id=\"Generate-the-training-set-5.7.1.1\"><span class=\"toc-item-num\">5.7.1.1&nbsp;&nbsp;</span>Generate the training set</a></span></li><li><span><a href=\"#Define-the-neural-network-model-to-be-used-and-train-it-using-the-training-set\" data-toc-modified-id=\"Define-the-neural-network-model-to-be-used-and-train-it-using-the-training-set-5.7.1.2\"><span class=\"toc-item-num\">5.7.1.2&nbsp;&nbsp;</span>Define the neural network model to be used and train it using the training set</a></span></li><li><span><a href=\"#Use-the-trained-model-to-make-predicttion-for-n_steps_ahead-time-increments\" data-toc-modified-id=\"Use-the-trained-model-to-make-predicttion-for-n_steps_ahead-time-increments-5.7.1.3\"><span class=\"toc-item-num\">5.7.1.3&nbsp;&nbsp;</span>Use the trained model to make predicttion for <em>n_steps_ahead</em> time increments</a></span></li></ul></li><li><span><a href=\"#Training-on-data-filtered-using-Gaussian-Mixtures\" data-toc-modified-id=\"Training-on-data-filtered-using-Gaussian-Mixtures-5.7.2\"><span class=\"toc-item-num\">5.7.2&nbsp;&nbsp;</span>Training on data filtered using Gaussian Mixtures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-the-training-set\" data-toc-modified-id=\"Generate-the-training-set-5.7.2.1\"><span class=\"toc-item-num\">5.7.2.1&nbsp;&nbsp;</span>Generate the training set</a></span></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-5.7.2.2\"><span class=\"toc-item-num\">5.7.2.2&nbsp;&nbsp;</span>Train the model</a></span></li><li><span><a href=\"#Use-the-trained-model-to-make-prediction-for-n_steps_ahead-time-increments\" data-toc-modified-id=\"Use-the-trained-model-to-make-prediction-for-n_steps_ahead-time-increments-5.7.2.3\"><span class=\"toc-item-num\">5.7.2.3&nbsp;&nbsp;</span>Use the trained model to make prediction for <em>n_steps_ahead</em> time increments</a></span></li></ul></li><li><span><a href=\"#Training-on-data-filtered-using-K-Means\" data-toc-modified-id=\"Training-on-data-filtered-using-K-Means-5.7.3\"><span class=\"toc-item-num\">5.7.3&nbsp;&nbsp;</span>Training on data filtered using K-Means</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-the-training-set\" data-toc-modified-id=\"Generate-the-training-set-5.7.3.1\"><span class=\"toc-item-num\">5.7.3.1&nbsp;&nbsp;</span>Generate the training set</a></span></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-5.7.3.2\"><span class=\"toc-item-num\">5.7.3.2&nbsp;&nbsp;</span>Train the model</a></span></li><li><span><a href=\"#Use-the-trained-model-to-make-prediction-for-n_steps_ahead-time-increments\" data-toc-modified-id=\"Use-the-trained-model-to-make-prediction-for-n_steps_ahead-time-increments-5.7.3.3\"><span class=\"toc-item-num\">5.7.3.3&nbsp;&nbsp;</span>Use the trained model to make prediction for <em>n_steps_ahead</em> time increments</a></span></li></ul></li><li><span><a href=\"#Plots-of-unfiltered-data\" data-toc-modified-id=\"Plots-of-unfiltered-data-5.7.4\"><span class=\"toc-item-num\">5.7.4&nbsp;&nbsp;</span>Plots of unfiltered data</a></span></li><li><span><a href=\"#Plots-of-Gaussian-Mixtures-cleaned-/-filtered-data\" data-toc-modified-id=\"Plots-of-Gaussian-Mixtures-cleaned-/-filtered-data-5.7.5\"><span class=\"toc-item-num\">5.7.5&nbsp;&nbsp;</span>Plots of Gaussian Mixtures cleaned / filtered data</a></span></li></ul></li><li><span><a href=\"#Compare-the-accuracy-of-the-prediction-made-by-the-the-different-trained-models\" data-toc-modified-id=\"Compare-the-accuracy-of-the-prediction-made-by-the-the-different-trained-models-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Compare the accuracy of the prediction made by the the different trained models</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-MSE-of-the-unmodified-data\" data-toc-modified-id=\"The-MSE-of-the-unmodified-data-5.8.1\"><span class=\"toc-item-num\">5.8.1&nbsp;&nbsp;</span>The MSE of the unmodified data</a></span></li><li><span><a href=\"#The-MSE-of-the-Gaussian-Mixtures-cleaned-data\" data-toc-modified-id=\"The-MSE-of-the-Gaussian-Mixtures-cleaned-data-5.8.2\"><span class=\"toc-item-num\">5.8.2&nbsp;&nbsp;</span>The MSE of the Gaussian Mixtures cleaned data</a></span></li><li><span><a href=\"#The-MSE-of-the-K-Means-cleaned-data\" data-toc-modified-id=\"The-MSE-of-the-K-Means-cleaned-data-5.8.3\"><span class=\"toc-item-num\">5.8.3&nbsp;&nbsp;</span>The MSE of the K-Means cleaned data</a></span></li><li><span><a href=\"#Percent-improvement-in-neural-network-predictions\" data-toc-modified-id=\"Percent-improvement-in-neural-network-predictions-5.8.4\"><span class=\"toc-item-num\">5.8.4&nbsp;&nbsp;</span>Percent improvement in neural network predictions</a></span></li></ul></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "This notebook demonstrates a process by which raw GNSS data obtained from a CHORDS portal can be processed with minimal human input, removing data points that are manifestations of high noise, instrumentation error, and other factors that introduce a larege error in to specific measurements. These prepared and cleaned data are then used to train a neural network that can be used for detecting real-time volcanic events.\n",
    "\n",
    "Add description of CHORDS and cite Daniels et al here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical contributions\n",
    "- Created a python-based API client to download data from CHORDS porals\n",
    "- Development of local libraries to download, manipultes, and plots GNSS data\n",
    "- Identification and removal of stastistical outliers in GNSS timeseries data using the Gaussian Mixtures Algoritm\n",
    "- Identification and removal of stastistical outliers in GNSS timeseries data using the K-means Algoritm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "- Select instrument and date range of data to process and analyse\n",
    "- Download selected data set from TZVOLCANO CHORDS portal\n",
    "- Scale and Impute data to prepare them from machine learning algorithms\n",
    "- For each native and calculated variable use a Gaussian Mixtures algorithm to identify and remove data points likely to have significant noise\n",
    "- Train two Neural networks: one using the unfiltered data, and one using the \"cleansed\" data\n",
    "- Use predictions made by the two nerual nets to make prediction of future data points\n",
    "- Compate these predictions to actual vaules to quantify the reduction in noise achieved by the filtering algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Application of the unsupervised learning algorithms significantly improves the reliability of the predictions a trained neural net can make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funding\n",
    "\n",
    "\n",
    "The development of this notebook was not directly supported by any awards, however the notebook leverages the EarthCube cyberinfrastructure CHORDS which was funded by the National Science Foundation.\n",
    "\n",
    "- Award1 = {\"agency\": \"US National Science Foundation\", \"award_code\": \"1440133\", \"award_URL\": \"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1440133&HistoricalAwards=false\"}\n",
    "- Award2 = {\"agency\": \"US National Science Foundation\", \"award_code\": \"1639750\", \"award_URL\": \"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1639750&HistoricalAwards=false\"}\n",
    "- Award3 = {\"agency\": \"US National Science Foundation\", \"award_code\": \"1639554\", \"award_URL\": \"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1639554&HistoricalAwards=false\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "keywords=[\"TZVOLCANO\", \"CHORDS\", \"Artificial Intelligence\", \"Machine Learning\", \"Earthcube\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "Include recommended citation for the notebook.\n",
    "\n",
    "For example: EarthCube Office, 2021. EarthCube Notebook Template. Accessed 2/1/2021 at https://github.com/earthcube/NotebookTemplates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work In Progress - improvements\n",
    "- Get ORCID for Myles\n",
    "- Update Results section\n",
    "- Add detailed text for the neural net processing and analysis sections\n",
    "- Add items ot suggested next steps\n",
    "- Verify the format of the Authos section\n",
    "- Remove commented out code from some sections\n",
    "- clean up the custom libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested next steps\n",
    "- A Support Vector Machine should be investigated as a possible filtering mechanism\n",
    "- CHORDS API should be made more robus and flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements \n",
    "\n",
    "**Need proper verbage here**\n",
    "\n",
    "- CHORDS\n",
    "- Virginia Tech\n",
    "- EarthCube & Earthcube Office"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "This notebook is licensed under the [MIT](License.md). Please refer to the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "A brief definition of these terms is provided below for terms that may be unfamilar to those without experience with machine learning, or are used in ways that may be unusual or ambiguous.\n",
    "\n",
    "- **[Feature](https://en.wikipedia.org/wiki/Feature_(machine_learning)**: \"a individual property or characteristic of a phenomenon being observed.\" In this notebook, the imported fields (Time, Height, Longitude, and Latitude) are the initial features. One additinal feature is calculated on the fly - the vector magnitude of scaled values of the original fields.\n",
    "\n",
    "- **Impute**: \n",
    "\n",
    "- **Anomaly**: Data that for varying reasons do not occur withing the usual ranges. In this notebook, there are (at least) two types of anomalies that may occur: those due to inaccurate measurements and subsequent processing, and those due to actual volcanic events. \n",
    "    \n",
    "Reference: [Wikipedia](https://en.wikipedia.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Library import\n",
    "Import all the required Python libraries.\n",
    "\n",
    "The code cell below is an example.\n",
    "When submitting your notebook, make sure that all external libraries are included in the requirements or environment file and library versions are explictly defined.\n",
    "\n",
    "It is a good practice to organize the imported libraries by functionality, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System functionality\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting and Vizualizations\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pipeline, scaling, normalizing, etc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# MSE calculation\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Algorithms\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Support\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "    \n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "    \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local library import\n",
    "Import all the required local libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Function definitions are included in this file to improve code readibility \n",
    "# \n",
    "# Many of these functions are based on code from the excellent book\n",
    "# Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\n",
    "# by Aurélien Géron\n",
    "#####\n",
    "\n",
    "# Include local library paths\n",
    "sys.path.append('./libraries')\n",
    "\n",
    "# Import local libraries\n",
    "\n",
    "# Misc. functions to support data analysis\n",
    "from TZVOLCANO_utilities import *\n",
    "from TZVOLCANO_plotting import *\n",
    "from TZVOLCANO_gaussian_mixtures import *\n",
    "from TZVOLCANO_kmeans import *\n",
    "from TZVOLCANO_neural_net import *\n",
    "\n",
    "# CHORDS GUI interface (uses ChordsAPI.py)\n",
    "from chords_gui import chords_gui "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter definitions\n",
    "Set all relevant parameters for the notebook. By convention, parameters are uppercase, while all the other variables follow Python's guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CHORDS GUI and load the API. DOMAIN is the URL of your CHORDS portal. \n",
    "DOMAIN = 'tzvolcano.chordsrt.com'\n",
    "CHORDS = chords_gui(DOMAIN)\n",
    "\n",
    "# Define the initial start and end date for the date selector\n",
    "INITIAL_START_DATE = '2021-01-01'\n",
    "INITIAL_END_DATE = '2021-01-01'\n",
    "\n",
    "\n",
    "# Min/max values used to scale the height, lon, and lat\n",
    "# This should be either -1, 1 or 0, 1\n",
    "SCALE_MINIMUM = 0\n",
    "SCALE_MAXIMUM = 1\n",
    "\n",
    "#####\n",
    "# Define important variables that control the neural net functionality.\n",
    "#####\n",
    "n_steps_training = 6 * 1000      # The number of data points to use in the training set\n",
    "n_steps_forecast = 500           # The number of data points to display in the predictions graph\n",
    "n_steps_ahead = 100              # the number of steps ahead that the neural net will predict\n",
    "\n",
    "\n",
    "# Gaussian Mixtures Parameters\n",
    "N_COMPONENTS = 1                 # The number of regions to generate - needs to be 1 for this use case\n",
    "N_INIT = 10                      \n",
    "COVARIANCE_TYPE = \"tied\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "\n",
    "# Data import\n",
    "\n",
    "Data for this notebook is stored in the [TZVOLCANO CHORDS portal](http://tzvolcano.chordsrt.com/). In order to run the notebook, users will need to select a date range, an instrument identifier, and click the \"Download File\" button. The designated data file is then downloaded to the local server and can be selected for use.\n",
    "\n",
    "The data is is CSV format, and is downloaded useing the CHORDS API.\n",
    "\n",
    "Each row of data includes a time, latitude, longitude, and height of instruments deployed on the Ol Doinyo Lengai vlcano for the TZVOLCANO initiative. Below is a map of the instrument IDs to the TZVOLCANO designators:\n",
    "```\n",
    "Instrument ID    Instrument Name    Notes\n",
    "1                OLO1               Live instrument\n",
    "2                OLO3               Live instrument\n",
    "4                OLOT               Test instrument with data from station BILL_RTX\n",
    "5                OLO6               Live instrument\n",
    "6                OLO7               Live instrument\n",
    "7                OLO8               Live instrument\n",
    "8                OLOJ               Test instrument \n",
    "9                OLON               Test instrument\n",
    "```\n",
    "\n",
    "The default for the notebook is to use OLO1 (instrument id 1). OLOT (instrument id 4) is a test site that contains data from station BILL_RTX. Data from other instruments may not be available for the default time window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note on imported data\n",
    "\n",
    "The entire date range is analyzed using the K-Means or Gaussian Mixtures clustering analysis. The large number of data points (millions of points) would, however, slow the training of the neural networks to an unusable pace when running on a server without GPU support (such as the servers used by mybinder). \n",
    "\n",
    "The **n_steps_training** variable limits the number of data points fed to the neural nework. 10,000 - 20,000 points seems to provide enough training data to make passable predictions and is still a small enough data set that training the net takes a resonable amount of time (several minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the instrument ID and the start and end date of the data to be processed and analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667febdaec234c1794ffd69f46bd532e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Select(description='Instrument ID: ', options=('1', '2', '4', '5', '6', '7', '8', '9'), value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71e77b213a247a4b4fa222a03557f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(DatePicker(value=datetime.datetime(2021, 1, 1, 0, 0), description='Start Date'), DatePicker(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957d8807d3894eac829ae673118b2268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Download File', style=ButtonStyle()),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09107ef4b0c423cb9dd9d73ecff29bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Textarea(value='', description='Output:', layout=Layout(height='100px', width='90%')),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a95ef818144b09ab2cb57d61d9723f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CHORDS.start_end_widgets(INITIAL_START_DATE, INITIAL_END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data file for use during analysis\n",
    "\n",
    "Choose which of the downloaded CSV files ot use for analysis. \n",
    "\n",
    "These files are retained on the server running the notebook indefinitely. \n",
    "\n",
    "**Note that on sharder servers (such as mybinder.org) very large files may not work properly if they consume all available disk space and or memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Data Files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a1b455aaad43e0be709b27e7d5fc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Select(layout=Layout(width='initial'), options=(), value=None),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a95ef818144b09ab2cb57d61d9723f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CHORDS.select_data_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read contents of the selected file in to a pandas object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bf27a64bb13a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCHORDS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_data_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Imported csv dat from\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0munmodified_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCHORDS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "file_name = CHORDS.available_data_files.value\n",
    "print(\"Imported csv dat from\" + file_name)\n",
    "\n",
    "unmodified_data = CHORDS.load_data_from_file(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and analysis\n",
    "The core of the notebook is here. Split this section into subsections as required, and explain processing and analysis steps.\n",
    "\n",
    "Selected best practices for organizing and formatting your notebooks are included below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Resample the data to fill in holes in the time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a copy of the unmodified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing points in the time series\n",
    "resampled_data = unmodified_data.copy()\n",
    "\n",
    "# Set the 'Time' field to be used as the index\n",
    "resampled_data = resampled_data.set_index('Time').sort_index()\n",
    "\n",
    "# Re-insert the 'Time' field, as changing it to be the index removes is as a referencable field in the pandas object\n",
    "resampled_data['Time'] = resampled_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative method: Make copy of unmodified data **and** resample the time series to fill in any missing time values in the series\n",
    "The standard approace for anomaly detection recomments imputing missing data points. Hoever, in this case the imputing approach was found to reduce the final accuracy of the predictions made by the neural network. The code below is commented out and is not executed. IT was retained as documentation in case the \"imputing\" approach is helpful for other applications of this overall methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the unmodified data\n",
    "# resampled_data = unmodified_data.copy()\n",
    "\n",
    "# Fill in missing points in the time series, inserting any times missing from the series using NaN as the value\n",
    "# resampled_data = resampled_data.set_index('Time').sort_index().resample('1000ms').ffill()\n",
    "\n",
    "# Re-insert the 'Time' field, as the resampling process changed it to be the index\n",
    "# resampled_data['Time'] = resampled_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale the Variables\n",
    "Many clustering algorithms are highly sensitive to min-max ranges - including both the K-means and Gaussian mixtures algorithms. Here a best practice is followed, and before running the clustering algorithms, the data are rescaled.\n",
    "\n",
    "Details of the scaling procedure can be found by examining the *scale_np_data* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale Height, Longitude and Latitude to the range between SCALE_MINIMUM and SCALE_MAXIMUM\n",
    "scaled_data = pd.DataFrame()\n",
    "\n",
    "# Convert the Time variable to Seconds Since Epoch\n",
    "scaled_data[\"Seconds Since Epoch\"] = resampled_data['Time'].astype(np.int64)\n",
    "# scaled_data[\"Time\"] = resampled_data['Time']\n",
    "\n",
    "scaled_data[\"Scaled Height\"] = scale_np_data(resampled_data[\"Height\"].to_numpy(), SCALE_MINIMUM, SCALE_MAXIMUM)\n",
    "scaled_data[\"Scaled Latitude\"] = scale_np_data(resampled_data[\"Latitude\"].to_numpy(), SCALE_MINIMUM, SCALE_MAXIMUM)\n",
    "scaled_data[\"Scaled Longitude\"] = scale_np_data(resampled_data[\"Longitude\"].to_numpy(), SCALE_MINIMUM, SCALE_MAXIMUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 10))\n",
    "\n",
    "# ax = plt.gca()  # get current axis\n",
    "# alpha = 0.6\n",
    "\n",
    "# scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Scaled Height', color='blue',ax=ax, alpha=alpha)\n",
    "# scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Scaled Latitude', color='red', ax=ax, alpha=alpha)\n",
    "# scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Scaled Longitude', color='orange', ax=ax, alpha=alpha)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Vector Magnitude\n",
    "Treating the individual fields as a vector, calculate the vector magnitude value as a derived feature.\n",
    "\n",
    "Creating an derived feature (variable) is a common technique in machine learning. It often makes it possible to more easily detect patterns in correlated variables. In this case, it makes it easier to identify regions of localized high-noise areas within the timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_list = ['Scaled Height', 'Scaled Latitude', 'Scaled Longitude']\n",
    "\n",
    "scaled_data[\"Vector Magnitude\"] = calculate_vector_magnitude(scaled_data, fields_list, SCALE_MINIMUM, SCALE_MAXIMUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Variables\n",
    "Note the the \"Vector Megnitude\" variable in green. It's wide range of values makes it easier for the algorithms to identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "ax = plt.gca() # get current axis\n",
    "alpha = 0.6\n",
    "\n",
    "scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Scaled Height', color='blue',ax=ax, alpha=alpha)\n",
    "scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Scaled Latitude', color='red', ax=ax, alpha=alpha)\n",
    "scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Scaled Longitude', color='orange', ax=ax, alpha=alpha)\n",
    "scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Vector Magnitude', color='green', ax=ax, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify outliers for each variable with the Gaussian Mixtures algorithm\n",
    "\n",
    "In order to save space and make the code more readable, several steps in this process have been abstracted out to functions in an external library.\n",
    "\n",
    "For each vaiable, the same processing and analysis is applied:\n",
    " - Designate which variable the is being used\n",
    " - Designate a *density threshold percent* that deterimines the percentage of data point that will be classified as being outliers\n",
    " - Perform transformations which scale and impute the data (see note) for the designated variable\n",
    " - Train a Gaussian Mixtures model with the imputed data\n",
    " - Generate a plot showing which points are flagged for removal\n",
    " \n",
    " A manual part of this process is performed at this point: Determining what value of the density threshold percent yields the best results. The values that appead in this code were arrived at my executing this process, looking at the generated plot, adjusting the \n",
    " density threshold percent, and the re-running this section of the code. This was performed until the plot indicates that outliers are identified, and that points that should be retained are not flagged for removal. \n",
    " \n",
    "Note: Imputing the data fills in missing values with the most frequently occurring value for that variable. This is necessary as the algorithm cannot function with missing values for times in the series. A number of these missing points were created, inserting DateTime values as necessary so that there were no gaps in the timeseries. When that operaton was performs, the values (E.g. for the height) were initilized as null/NaN. Imputing sets those NaN values to be the most frequent value for that variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers for the Scaled Height variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'Scaled Height'\n",
    "scaled_height_density_threshold_percent = 5.2\n",
    "\n",
    "scaled_height_data_imputed = transform_data_for_gaussian_mixtures(scaled_data, field_name)\n",
    "\n",
    "scaled_height_gm, scaled_height_anomalies = get_anomalies_using_gaussian_mixtures(scaled_height_data_imputed, scaled_height_density_threshold_percent)\n",
    "\n",
    "plot_gaussian_mixture_anomalies(scaled_height_gm, scaled_height_data_imputed, scaled_height_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers for the Scaled Latitude variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'Scaled Latitude'\n",
    "scaled_latitude_density_threshold_percent = 3\n",
    "\n",
    "scaled_latitude_data_imputed = transform_data_for_gaussian_mixtures(scaled_data, field_name)\n",
    "\n",
    "scaled_latitude_gm, scaled_latitude_anomalies = get_anomalies_using_gaussian_mixtures(scaled_latitude_data_imputed, scaled_latitude_density_threshold_percent)\n",
    "\n",
    "plot_gaussian_mixture_anomalies(scaled_latitude_gm, scaled_latitude_data_imputed, scaled_latitude_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers for the Scaled Longitude variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'Scaled Longitude'\n",
    "scaled_longitude_density_threshold_percent = 7\n",
    "\n",
    "scaled_longitude_data_imputed = transform_data_for_gaussian_mixtures(scaled_data, field_name)\n",
    "\n",
    "scaled_longitude_gm, scaled_longitude_anomalies = get_anomalies_using_gaussian_mixtures(scaled_longitude_data_imputed, scaled_longitude_density_threshold_percent)\n",
    "\n",
    "plot_gaussian_mixture_anomalies(scaled_longitude_gm, scaled_longitude_data_imputed, scaled_longitude_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers for the Vector Magnitude variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'Vector Magnitude'\n",
    "vector_magnitude_density_threshold_percent = 7\n",
    "\n",
    "vector_magnitude_data_imputed = transform_data_for_gaussian_mixtures(scaled_data, field_name)\n",
    "\n",
    "vector_magnitude_gm, vector_magnitude_anomalies = get_anomalies_using_gaussian_mixtures(vector_magnitude_data_imputed, vector_magnitude_density_threshold_percent)\n",
    "\n",
    "plot_gaussian_mixture_anomalies(vector_magnitude_gm, vector_magnitude_data_imputed, vector_magnitude_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate the arrays of times to remove in to one object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list of anomalies to dateime data type so they can be used to filter the scaled data set\n",
    "scaled_height_times_to_remove    = pd.to_datetime(scaled_height_anomalies[:, 0], unit='ns', utc=True)\n",
    "scaled_longitude_times_to_remove = pd.to_datetime(scaled_longitude_anomalies[:, 0], unit='ns', utc=True)\n",
    "scaled_latitude_times_to_remove  = pd.to_datetime(scaled_latitude_anomalies[:, 0], unit='ns', utc=True)\n",
    "vector_magnitude_times_to_remove = pd.to_datetime(vector_magnitude_anomalies[:, 0], unit='ns', utc=True)\n",
    "\n",
    "\n",
    "# Consolidate the arrays of times to remove in to one object\n",
    "times_to_remove = scaled_height_times_to_remove\n",
    "times_to_remove = times_to_remove.union(scaled_longitude_times_to_remove)\n",
    "times_to_remove = times_to_remove.union(scaled_latitude_times_to_remove)\n",
    "times_to_remove = times_to_remove.union(vector_magnitude_times_to_remove)\n",
    "times_to_remove = times_to_remove.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new pandas object with the points identified by the algorith removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the flagged times from the scaled data set \n",
    "g_m_cleaned_data = scaled_data.copy()\n",
    "g_m_cleaned_data = g_m_cleaned_data.drop(times_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the \"cleansed\" data on top of the orginal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# get current axis\n",
    "ax = plt.gca()\n",
    "alpha = 0.99\n",
    "\n",
    "\n",
    "scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Vector Magnitude', color='blue',ax=ax, alpha=alpha)\n",
    "g_m_cleaned_data.plot(kind='line',x='Seconds Since Epoch',y='Vector Magnitude', color='red',ax=ax, alpha=alpha)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above show that the value ranges of the filtered data is much less than seen in the unmodified data. This is a good indicator that training a machine learning algorith (such as a neural net) should result in trained alrorithm that produces more accurate predictions than were it to be trained using the unfiltered data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify anomalies for each variable using the K-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find outliers for the Scaled Height variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_to_analyze = 'Scaled Height'\n",
    "number_of_clusters = 10\n",
    "\n",
    "# Impute the data \n",
    "scaled_height_kmeans_data_imputed = transform_data_for_kmeans(scaled_data, field_to_analyze)\n",
    "\n",
    "# Train the Kmeans model\n",
    "scaled_height_kmeans = KMeans(n_clusters=number_of_clusters, random_state=42)\n",
    "\n",
    "scaled_height_cluster_labels = scaled_height_kmeans.fit_predict(scaled_height_kmeans_data_imputed)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_decision_boundaries(scaled_height_kmeans, scaled_height_kmeans_data_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the labeled regions from the decision boundary plot, decide which regions should be retained\n",
    "scaled_height_regions_to_retain = [4,1,5,3,0,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find outliers for the Scaled Latitude variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Kmeans cluster \n",
    "field_to_analyze = 'Scaled Latitude'\n",
    "number_of_clusters = 5\n",
    "\n",
    "# Impute and scale the data \n",
    "scaled_latitude_kmeans_imputed_data = transform_data_for_kmeans(scaled_data, field_to_analyze)\n",
    "\n",
    "# Train the Kmeans model\n",
    "scaled_latitude_kmeans = KMeans(n_clusters=number_of_clusters, random_state=42)\n",
    "\n",
    "scaled_latitude_cluster_labels = scaled_latitude_kmeans.fit_predict(scaled_latitude_kmeans_imputed_data)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_decision_boundaries(scaled_latitude_kmeans, scaled_latitude_kmeans_imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the labeled regions from the decision boundary plot, decide which regions should be retained\n",
    "scaled_latitude_regions_to_retain = [1,3,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find outliers for the Scaled Longitude variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_to_analyze = 'Scaled Longitude'\n",
    "number_of_clusters = 5\n",
    "\n",
    "\n",
    "# Impute the data \n",
    "scaled_longitude_kmeans_data_imputed = transform_data_for_kmeans(scaled_data, field_to_analyze)\n",
    "\n",
    "# Train the Kmeans model\n",
    "scaled_longitude_kmeans = KMeans(n_clusters=number_of_clusters, random_state=41)\n",
    "\n",
    "scaled_longitude_cluster_labels = scaled_longitude_kmeans.fit_predict(scaled_longitude_kmeans_data_imputed)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_decision_boundaries(scaled_longitude_kmeans, scaled_longitude_kmeans_data_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the labeled regions from the decision boundary plot, decide which regions should be retained\n",
    "scaled_longitude_regions_to_retain = [3,0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find outliers for the Vector Magnitude variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_to_analyze = 'Vector Magnitude'\n",
    "number_of_clusters = 10\n",
    "\n",
    "# Impute the data \n",
    "vector_magnitude_kmeans_data_imputed = transform_data_for_kmeans(scaled_data, field_to_analyze)\n",
    "\n",
    "# Train the Kmeans model\n",
    "vector_magnitude_kmeans = KMeans(n_clusters=number_of_clusters, random_state=42)\n",
    "\n",
    "vector_magnitude_cluster_labels = vector_magnitude_kmeans.fit_predict(vector_magnitude_kmeans_data_imputed)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_decision_boundaries(vector_magnitude_kmeans, vector_magnitude_kmeans_data_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the labeled regions from the decision boundary plot, decide which regions should be retained\n",
    "vector_magnitude_regions_to_retain = [4,1,6,3,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectively remove data points from the list\n",
    "\n",
    "# Create a new containerd for the cleaned data\n",
    "# cleaned_kmeans_data = pd.DataFrame()\n",
    "\n",
    "# Convert the Time variable to Seconds Since Epoch\n",
    "# cleaned_kmeans_data[\"Seconds Since Epoch\"] = resampled_data['Time'].astype(np.int64)\n",
    "\n",
    "cleaned_kmeans_data = scaled_data\n",
    "\n",
    "\n",
    "# cleaned_kmeans_data[\"Height\"] = scaled_data[\"Height\"]\n",
    "# cleaned_kmeans_data[\"Longitude\"] = scaled_data[\"Longitude\"]\n",
    "# cleaned_kmeans_data[\"Latitude\"] = scaled_data[\"Latitude\"]\n",
    "# cleaned_kmeans_data[\"Vector Magnitude\"] = scaled_data[\"Vector Magnitude\"]\n",
    "\n",
    "\n",
    "# Create a new column in the panadas object for the cluster labels for each dimension\n",
    "cleaned_kmeans_data['scaled_height_kmeans'] = scaled_height_cluster_labels\n",
    "cleaned_kmeans_data['scaled_longitude_kmeans'] = scaled_longitude_cluster_labels\n",
    "cleaned_kmeans_data['scaled_latitude_kmeans'] = scaled_latitude_cluster_labels\n",
    "cleaned_kmeans_data['vector_magnitude_kmeans'] = vector_magnitude_cluster_labels\n",
    "\n",
    "\n",
    "# For each dimension, keep only the designated rows\n",
    "cleaned_kmeans_data = cleaned_kmeans_data[cleaned_kmeans_data.scaled_height_kmeans.isin(scaled_height_regions_to_retain)]\n",
    "cleaned_kmeans_data = cleaned_kmeans_data[cleaned_kmeans_data.scaled_longitude_kmeans.isin(scaled_longitude_regions_to_retain)]\n",
    "cleaned_kmeans_data = cleaned_kmeans_data[cleaned_kmeans_data.scaled_latitude_kmeans.isin(scaled_latitude_regions_to_retain)]\n",
    "cleaned_kmeans_data = cleaned_kmeans_data[cleaned_kmeans_data.vector_magnitude_kmeans.isin(vector_magnitude_regions_to_retain)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Unfiltered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data in the proper format to train the RNN\n",
    "# X_train, y_train = get_RNN_training_sets(scaled_data, 'Scaled Height', n_steps_training)\n",
    "\n",
    "RNN_training_data, RNN_training_labels = get_RNN_training_sets(scaled_data, 'Scaled Height', n_steps_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the neural network model to be used and train it using the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_neural_net_model(n_steps_ahead)\n",
    "\n",
    "history = model.fit(RNN_training_data, RNN_training_labels, epochs=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Take a look at the training set\n",
    "# print(RNN_training_labels.shape)\n",
    "# print(RNN_training_labels.shape)\n",
    "# plot_series(RNN_training_labels[0, :, 0], n_steps_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the trained model to make predicttion for *n_steps_ahead* time increments \n",
    "Only use the number of points for the predictions that were used for training the neural net (plus the number of steps ahead that we want to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_records = n_steps_training + n_steps_ahead\n",
    "truncated_scaled_data = scaled_data.head(number_of_records)\n",
    "\n",
    "# truncated_scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Scaled Height', color='blue')\n",
    "\n",
    "X_new, Y_new = get_RNN_forecast_sets(truncated_scaled_data, 'Scaled Height', n_steps_training, n_steps_forecast, n_steps_ahead )\n",
    "\n",
    "X = X_new\n",
    "\n",
    "Y_pred = model.predict(X_new)[:, -1][..., np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the entire training set \n",
    "# plot_series(RNN_training_labels[0, :, 0], n_steps_training)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot the last n_step_forecast\n",
    "# plot_multiple_forecasts(X_new, Y_new, Y_pred, n_steps_forecast)\n",
    "# # save_fig(\"forecast_ahead_plot\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on data filtered using Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_RNN_training_data, cleaned_RNN_training_labels = get_RNN_training_sets(g_m_cleaned_data, 'Scaled Height', n_steps_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new set of RNN training data based on the cleansed data\n",
    "cleaned_model = get_neural_net_model(n_steps_ahead)\n",
    "\n",
    "cleaned_history = cleaned_model.fit(cleaned_RNN_training_data, cleaned_RNN_training_labels, epochs=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cleaned_RNN_training_data.shape)\n",
    "print(cleaned_RNN_training_labels.shape)\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_series(cleaned_RNN_training_data[0, :, 0], n_steps_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the trained model to make prediction for *n_steps_ahead* time increments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_records = n_steps_training + n_steps_ahead\n",
    "truncated_g_m_cleaned_data = g_m_cleaned_data.head(number_of_records)\n",
    "\n",
    "X_cleaned_new, Y_cleaned_new = get_RNN_forecast_sets(truncated_g_m_cleaned_data, 'Scaled Height', n_steps_training-1, n_steps_forecast, n_steps_ahead)\n",
    "\n",
    "X_cleaned = X_cleaned_new\n",
    "\n",
    "Y_cleaned_pred = cleaned_model.predict(X_cleaned_new)[:, -1][..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on data filtered using K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cleaned_RNN_training_data, kmeans_cleaned_RNN_training_labels = get_RNN_training_sets(cleaned_kmeans_data, 'Scaled Height', n_steps_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new set of RNN training data based on the cleansed data\n",
    "kmeans_cleaned_model = get_neural_net_model(n_steps_ahead)\n",
    "\n",
    "kmeans_cleaned_history = kmeans_cleaned_model.fit(kmeans_cleaned_RNN_training_data, kmeans_cleaned_RNN_training_labels, epochs=20,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the trained model to make prediction for *n_steps_ahead* time increments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_records = n_steps_training + n_steps_ahead\n",
    "truncated_kmeans_cleaned_data = cleaned_kmeans_data.head(number_of_records)\n",
    "\n",
    "X_kmeans_cleaned_new, Y_kmeans_cleaned_new = get_RNN_forecast_sets(truncated_kmeans_cleaned_data, 'Scaled Height', n_steps_training-1, n_steps_forecast, n_steps_ahead)\n",
    "\n",
    "X_kmeans_cleaned = X_kmeans_cleaned_new\n",
    "\n",
    "Y_kmeans_cleaned_pred = kmeans_cleaned_model.predict(X_kmeans_cleaned_new)[:, -1][..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of unfiltered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the entire training set \n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_series(RNN_training_data[0, :, 0], n_steps_training)\n",
    "plt.show()\n",
    "\n",
    "# Plot the last n_step_forecast\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_multiple_forecasts(X_new, Y_new, Y_pred, n_steps_forecast)\n",
    "# save_fig(\"forecast_ahead_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of Gaussian Mixtures cleaned / filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the entire training set for comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_series(cleaned_RNN_training_data[0, :, 0], n_steps_training)\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_multiple_forecasts(X_cleaned_new, Y_cleaned_new, Y_cleaned_pred, n_steps_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptiong and analysis of the graphs\n",
    "pointing out what the forecast points mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots of K-means cleaned / filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the entire training set for comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_series(kmeans_cleaned_RNN_training_data[0, :, 0], n_steps_training)\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_multiple_forecasts(X_kmeans_cleaned_new, Y_kmeans_cleaned_new, Y_kmeans_cleaned_pred, n_steps_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the accuracy of the prediction made by the the different trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MSE of the unmodified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(Y_new.flatten(), Y_pred.flatten())\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MSE of the Gaussian Mixtures cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_m_cleaned_mse = mean_squared_error(Y_cleaned_new.flatten(), Y_cleaned_pred.flatten())\n",
    "g_m_cleaned_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MSE of the K-Means cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cleaned_mse = mean_squared_error(Y_kmeans_cleaned_new.flatten(), Y_kmeans_cleaned_pred.flatten())\n",
    "kmeans_cleaned_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percent improvement in neural network predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_m_diff = (g_m_cleaned_mse/mse)*100\n",
    "kmeans_diff = (kmeans_cleaned_mse/mse)*100\n",
    "\n",
    "print(\"For this run, the removal of the identified data points  results in a \")\n",
    "print(str(round(100-g_m_diff)) + \"% (Gaussian Mixtures)\")\n",
    "print(str(round(100-kmeans_diff)) + \"% (K-means)\")\n",
    "print(\"reduced error rate in predicting values using the trained neural net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2nd ed.). O'Reilly.\n",
    "\n",
    "Wikimedia Foundation. (2021, February 3). Wikipedia. https://en.wikipedia.org/. \n",
    "\n",
    "Daniels et al (CHORDS)\n",
    "\n",
    "Stamps 2016 TZVOLCANO for rt data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.182px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
