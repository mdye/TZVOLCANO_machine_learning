{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toward autonomous detection of anomalous GNSS data via applied unsupervised artificial intelligence \n",
    "\n",
    "Unsupervised Anomaly Detection of TZVOLCANO GNSS Data using Gaussian Mixtures. Once loaded in Binder, please run all the cells to properly initialize values and GUI elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Mike Dye, D. Sarah Stamps, Myles Mason\n",
    "\n",
    "- Author1 = {\"name\": \"Mike Dye\", \"affiliation\": \"Unaffiliated\", \"email\": \"mike@mikedye.com\", \"orcid\": \"0000-0003-2065-870X\"}\n",
    "- Author2 = {\"name\": \"Dr. Sarah Stamps\", \"affiliation\": \"Virginia Tech\", \"email\": \"dstamps@vt.edu\", \"orcid\": \"0000-0002-3531-1752\"}\n",
    "- Author3 = {\"name\": \"Myles Mason\", \"affiliation\": \"Virginia Tech\", \"email\": \"mylesm18@vt.edu\", \"orcid\": \"0000-0002-8811-8294\"}\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Toward-autonomous-detection-of-anomalous-GNSS-data-via-applied-unsupervised-artificial-intelligence\" data-toc-modified-id=\"Toward-autonomous-detection-of-anomalous-GNSS-data-via-applied-unsupervised-artificial-intelligence-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Toward autonomous detection of anomalous GNSS data via applied unsupervised artificial intelligence</a></span><ul class=\"toc-item\"><li><span><a href=\"#Authors\" data-toc-modified-id=\"Authors-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Authors</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Technical-contributions\" data-toc-modified-id=\"Technical-contributions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Technical contributions</a></span></li><li><span><a href=\"#Methodology\" data-toc-modified-id=\"Methodology-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Methodology</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Results</a></span></li><li><span><a href=\"#Funding\" data-toc-modified-id=\"Funding-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Funding</a></span></li><li><span><a href=\"#Keywords\" data-toc-modified-id=\"Keywords-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Keywords</a></span></li><li><span><a href=\"#Citation\" data-toc-modified-id=\"Citation-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Citation</a></span></li><li><span><a href=\"#Work-In-Progress---improvements\" data-toc-modified-id=\"Work-In-Progress---improvements-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Work In Progress - improvements</a></span></li><li><span><a href=\"#Suggested-next-steps\" data-toc-modified-id=\"Suggested-next-steps-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Suggested next steps</a></span></li><li><span><a href=\"#Acknowledgements\" data-toc-modified-id=\"Acknowledgements-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Acknowledgements</a></span></li><li><span><a href=\"#License\" data-toc-modified-id=\"License-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>License</a></span></li><li><span><a href=\"#Glossary\" data-toc-modified-id=\"Glossary-1.13\"><span class=\"toc-item-num\">1.13&nbsp;&nbsp;</span>Glossary</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Library-import\" data-toc-modified-id=\"Library-import-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Library import</a></span></li><li><span><a href=\"#Local-library-import\" data-toc-modified-id=\"Local-library-import-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Local library import</a></span></li></ul></li><li><span><a href=\"#Parameter-definitions\" data-toc-modified-id=\"Parameter-definitions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Parameter definitions</a></span></li><li><span><a href=\"#Data-import\" data-toc-modified-id=\"Data-import-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data import</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Important-note-on-imported-data\" data-toc-modified-id=\"Important-note-on-imported-data-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Important note on imported data</a></span></li></ul></li><li><span><a href=\"#Select-the-instrument-ID-and-the-start-and-end-date-of-the-data-to-be-processed-and-analyzed\" data-toc-modified-id=\"Select-the-instrument-ID-and-the-start-and-end-date-of-the-data-to-be-processed-and-analyzed-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Select the instrument ID and the start and end date of the data to be processed and analyzed</a></span></li><li><span><a href=\"#Select-data-file-for-use-during-analysis\" data-toc-modified-id=\"Select-data-file-for-use-during-analysis-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Select data file for use during analysis</a></span></li><li><span><a href=\"#Read-contents-of-the-selected-file-in-to-a-pandas-object\" data-toc-modified-id=\"Read-contents-of-the-selected-file-in-to-a-pandas-object-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Read contents of the selected file in to a pandas object</a></span></li></ul></li><li><span><a href=\"#Data-processing-and-analysis\" data-toc-modified-id=\"Data-processing-and-analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data processing and analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Resample-the-data-to-fill-in-holes-in-the-time-series\" data-toc-modified-id=\"Resample-the-data-to-fill-in-holes-in-the-time-series-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Resample the data to fill in holes in the time series</a></span><ul class=\"toc-item\"><li><span><a href=\"#Make-a-copy-of-the-unmodified-data\" data-toc-modified-id=\"Make-a-copy-of-the-unmodified-data-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Make a copy of the unmodified data</a></span></li><li><span><a href=\"#Alternative-method:-Make-copy-of-unmodified-data-and-resample-the-time-series-to-fill-in-any-missing-time-values-in-the-series\" data-toc-modified-id=\"Alternative-method:-Make-copy-of-unmodified-data-and-resample-the-time-series-to-fill-in-any-missing-time-values-in-the-series-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Alternative method: Make copy of unmodified data <strong>and</strong> resample the time series to fill in any missing time values in the series</a></span></li></ul></li><li><span><a href=\"#Rescale-the-Variables\" data-toc-modified-id=\"Rescale-the-Variables-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Rescale the Variables</a></span></li><li><span><a href=\"#Calculate-the-Vector-Magnitude\" data-toc-modified-id=\"Calculate-the-Vector-Magnitude-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Calculate the Vector Magnitude</a></span></li><li><span><a href=\"#Plot-the-Variables\" data-toc-modified-id=\"Plot-the-Variables-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Plot the Variables</a></span></li><li><span><a href=\"#Identify-outliers-for-each-variable-with-the-Gaussian-Mixtures-algorithm\" data-toc-modified-id=\"Identify-outliers-for-each-variable-with-the-Gaussian-Mixtures-algorithm-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Identify outliers for each variable with the Gaussian Mixtures algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-outliers-for-the-Scaled-Height-variable\" data-toc-modified-id=\"Find-outliers-for-the-Scaled-Height-variable-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>Find outliers for the Scaled Height variable</a></span></li><li><span><a href=\"#Find-outliers-for-the-Scaled-Latitude-variable\" data-toc-modified-id=\"Find-outliers-for-the-Scaled-Latitude-variable-5.5.2\"><span class=\"toc-item-num\">5.5.2&nbsp;&nbsp;</span>Find outliers for the Scaled Latitude variable</a></span></li><li><span><a href=\"#Find-outliers-for-the-Scaled-Longitude-variable\" data-toc-modified-id=\"Find-outliers-for-the-Scaled-Longitude-variable-5.5.3\"><span class=\"toc-item-num\">5.5.3&nbsp;&nbsp;</span>Find outliers for the Scaled Longitude variable</a></span></li><li><span><a href=\"#Find-outliers-for-the-Vector-Magnitude-variable\" data-toc-modified-id=\"Find-outliers-for-the-Vector-Magnitude-variable-5.5.4\"><span class=\"toc-item-num\">5.5.4&nbsp;&nbsp;</span>Find outliers for the Vector Magnitude variable</a></span></li><li><span><a href=\"#Consolidate-the-arrays-of-times-to-remove-in-to-one-object\" data-toc-modified-id=\"Consolidate-the-arrays-of-times-to-remove-in-to-one-object-5.5.5\"><span class=\"toc-item-num\">5.5.5&nbsp;&nbsp;</span>Consolidate the arrays of times to remove in to one object</a></span></li><li><span><a href=\"#Create-a-new-pandas-object-with-the-points-identified-by-the-algorith-removed\" data-toc-modified-id=\"Create-a-new-pandas-object-with-the-points-identified-by-the-algorith-removed-5.5.6\"><span class=\"toc-item-num\">5.5.6&nbsp;&nbsp;</span>Create a new pandas object with the points identified by the algorithm removed</a></span></li><li><span><a href=\"#Plot-the-&quot;cleansed&quot;-data-on-top-of-the-orginal-data\" data-toc-modified-id=\"Plot-the-&quot;cleansed&quot;-data-on-top-of-the-orginal-data-5.5.7\"><span class=\"toc-item-num\">5.5.7&nbsp;&nbsp;</span>Plot the \"cleansed\" data on top of the orginal data</a></span></li></ul></li><li><span><a href=\"#Identify-anomalies-for-each-variable-using-the-K-means-algorithm\" data-toc-modified-id=\"Identify-anomalies-for-each-variable-using-the-K-means-algorithm-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Identify anomalies for each variable using the K-means algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#find-outliers-for-the-Scaled-Height-variable\" data-toc-modified-id=\"find-outliers-for-the-Scaled-Height-variable-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>find outliers for the Scaled Height variable</a></span></li><li><span><a href=\"#find-outliers-for-the-Scaled-Latitude-variable\" data-toc-modified-id=\"find-outliers-for-the-Scaled-Latitude-variable-5.6.2\"><span class=\"toc-item-num\">5.6.2&nbsp;&nbsp;</span>find outliers for the Scaled Latitude variable</a></span></li><li><span><a href=\"#find-outliers-for-the-Scaled-Longitude-variable\" data-toc-modified-id=\"find-outliers-for-the-Scaled-Longitude-variable-5.6.3\"><span class=\"toc-item-num\">5.6.3&nbsp;&nbsp;</span>find outliers for the Scaled Longitude variable</a></span></li><li><span><a href=\"#find-outliers-for-the-Vector-Magnitude-variable\" data-toc-modified-id=\"find-outliers-for-the-Vector-Magnitude-variable-5.6.4\"><span class=\"toc-item-num\">5.6.4&nbsp;&nbsp;</span>find outliers for the Vector Magnitude variable</a></span></li></ul></li><li><span><a href=\"#Training-the-neural-net\" data-toc-modified-id=\"Training-the-neural-net-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Training the neural net</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-on-Unfiltered-data\" data-toc-modified-id=\"Training-on-Unfiltered-data-5.7.1\"><span class=\"toc-item-num\">5.7.1&nbsp;&nbsp;</span>Training on Unfiltered data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-the-training-set\" data-toc-modified-id=\"Generate-the-training-set-5.7.1.1\"><span class=\"toc-item-num\">5.7.1.1&nbsp;&nbsp;</span>Generate the training set</a></span></li><li><span><a href=\"#Define-the-neural-network-model-to-be-used-and-train-it-using-the-training-set\" data-toc-modified-id=\"Define-the-neural-network-model-to-be-used-and-train-it-using-the-training-set-5.7.1.2\"><span class=\"toc-item-num\">5.7.1.2&nbsp;&nbsp;</span>Define the neural network model to be used and train it using the training set</a></span></li><li><span><a href=\"#Use-the-trained-model-to-make-predicttion-for-n_steps_ahead-time-increments\" data-toc-modified-id=\"Use-the-trained-model-to-make-predicttion-for-n_steps_ahead-time-increments-5.7.1.3\"><span class=\"toc-item-num\">5.7.1.3&nbsp;&nbsp;</span>Use the trained model to make predicttion for <em>n_steps_ahead</em> time increments</a></span></li></ul></li><li><span><a href=\"#Training-on-data-filtered-using-Gaussian-Mixtures\" data-toc-modified-id=\"Training-on-data-filtered-using-Gaussian-Mixtures-5.7.2\"><span class=\"toc-item-num\">5.7.2&nbsp;&nbsp;</span>Training on data filtered using Gaussian Mixtures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-the-training-set\" data-toc-modified-id=\"Generate-the-training-set-5.7.2.1\"><span class=\"toc-item-num\">5.7.2.1&nbsp;&nbsp;</span>Generate the training set</a></span></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-5.7.2.2\"><span class=\"toc-item-num\">5.7.2.2&nbsp;&nbsp;</span>Train the model</a></span></li><li><span><a href=\"#Use-the-trained-model-to-make-prediction-for-n_steps_ahead-time-increments\" data-toc-modified-id=\"Use-the-trained-model-to-make-prediction-for-n_steps_ahead-time-increments-5.7.2.3\"><span class=\"toc-item-num\">5.7.2.3&nbsp;&nbsp;</span>Use the trained model to make prediction for <em>n_steps_ahead</em> time increments</a></span></li></ul></li><li><span><a href=\"#Training-on-data-filtered-using-K-Means\" data-toc-modified-id=\"Training-on-data-filtered-using-K-Means-5.7.3\"><span class=\"toc-item-num\">5.7.3&nbsp;&nbsp;</span>Training on data filtered using K-Means</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-the-training-set\" data-toc-modified-id=\"Generate-the-training-set-5.7.3.1\"><span class=\"toc-item-num\">5.7.3.1&nbsp;&nbsp;</span>Generate the training set</a></span></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-5.7.3.2\"><span class=\"toc-item-num\">5.7.3.2&nbsp;&nbsp;</span>Train the model</a></span></li><li><span><a href=\"#Use-the-trained-model-to-make-prediction-for-n_steps_ahead-time-increments\" data-toc-modified-id=\"Use-the-trained-model-to-make-prediction-for-n_steps_ahead-time-increments-5.7.3.3\"><span class=\"toc-item-num\">5.7.3.3&nbsp;&nbsp;</span>Use the trained model to make prediction for <em>n_steps_ahead</em> time increments</a></span></li></ul></li><li><span><a href=\"#Plots-of-unfiltered-data\" data-toc-modified-id=\"Plots-of-unfiltered-data-5.7.4\"><span class=\"toc-item-num\">5.7.4&nbsp;&nbsp;</span>Plots of unfiltered data</a></span></li><li><span><a href=\"#Plots-of-Gaussian-Mixtures-cleaned-/-filtered-data\" data-toc-modified-id=\"Plots-of-Gaussian-Mixtures-cleaned-/-filtered-data-5.7.5\"><span class=\"toc-item-num\">5.7.5&nbsp;&nbsp;</span>Plots of Gaussian Mixtures cleaned / filtered data</a></span></li></ul></li><li><span><a href=\"#Compare-the-accuracy-of-the-prediction-made-by-the-the-different-trained-models\" data-toc-modified-id=\"Compare-the-accuracy-of-the-prediction-made-by-the-the-different-trained-models-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Compare the accuracy of the prediction made by the the different trained models</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-MSE-of-the-unmodified-data\" data-toc-modified-id=\"The-MSE-of-the-unmodified-data-5.8.1\"><span class=\"toc-item-num\">5.8.1&nbsp;&nbsp;</span>The MSE of the unmodified data</a></span></li><li><span><a href=\"#The-MSE-of-the-Gaussian-Mixtures-cleaned-data\" data-toc-modified-id=\"The-MSE-of-the-Gaussian-Mixtures-cleaned-data-5.8.2\"><span class=\"toc-item-num\">5.8.2&nbsp;&nbsp;</span>The MSE of the Gaussian Mixtures cleaned data</a></span></li><li><span><a href=\"#The-MSE-of-the-K-Means-cleaned-data\" data-toc-modified-id=\"The-MSE-of-the-K-Means-cleaned-data-5.8.3\"><span class=\"toc-item-num\">5.8.3&nbsp;&nbsp;</span>The MSE of the K-Means cleaned data</a></span></li><li><span><a href=\"#Percent-improvement-in-neural-network-predictions\" data-toc-modified-id=\"Percent-improvement-in-neural-network-predictions-5.8.4\"><span class=\"toc-item-num\">5.8.4&nbsp;&nbsp;</span>Percent improvement in neural network predictions</a></span></li></ul></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "This notebook demonstrates a process by which GNSS data (lontitude, latitude, and height) obtained from the TZVOLCANO CHORDS portal (Stamps et al., 2016) can be analyzed with minimal human input to remove data points that are manifestations of high noise, instrumentation error, and other factors that introduce large errors into specific measurements. These prepared and cleaned data are then used to train a neural network that can be used for detecting volcanic activity.\n",
    "\n",
    "This notebook takes advantage of the Earthcube funded CHORDS infrastructure (Daniels et al., 2016; Kerkez et al., 2016), which powers the TZVOLCANO CHORDS portal. GNSS positioning data (longitude, latitude, and height) are from the active Ol Doinyo Lengai volcano in Tanzania, which are made available through UNAVCO’s real-time GNSS data services. UNAVCO's real-time GNSS data services provides real-time positions processed by the Trimble Pivot system. Real-time GNSS data from several instruments are streamed into the TZVOLCANO portal using brokering scripts developed by Joshua Robert Jones in Python and D. Sarah Stamps in awk, which makes them instantly available via the CHORDS data API service.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical contributions\n",
    "- Created a python-based API client to download data from a CHORDS portal\n",
    "- Development of local libraries to download, manipulate, and plot GNSS data (longitude, latitude, and height) obtained from a CHORDS portal that obtains positions from UNAVCO's real-time GNSS data services\n",
    "- Identification and removal of statistical outliers in GNSS time-series data using the Gaussian Mixtures Algorithm\n",
    "- Identification and removal of statistical outliers in GNSS time-series data using the K-means Algorithm\n",
    "- Implementation of a neural net model which, when trained on these data, can make predictions based on the the historical time-series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "- Select instrument and date range of positioning data (longitude, latitude, and height) to analyze\n",
    "- Download selected data set from TZVOLCANO CHORDS portal\n",
    "- Scale and impute data to prepare them for machine learning algorithms\n",
    "- Use a Gaussian Mixtures and then a K-means algorithm to identify and remove data points likely to have significant noise from each feature/variable\n",
    "- Train three Neural networks: one using the unfiltered data and two using the \"cleaned\" data output from the Gaussian mixtures and K-Means algorithm\n",
    "- Use predictions made by the these neural nets to make predictions (forecasts) of future data points\n",
    "- Compare these predictions to actual values from the unmodified data set to quantify the reduction in noise achieved by the filtering algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Compared to the neural net trained on the unfiltered data, filtered (or \"cleaned\") data output Machine Learning algorithsm (Gaussian Mixtures and K-means) result in trained neural net models that do a significantly better job of generating predictions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funding\n",
    "\n",
    "\n",
    "The development of this notebook was not directly supported by any awards, however the notebook leverages the EarthCube cyberinfrastructure CHORDS which was funded by the National Science Foundation.\n",
    "\n",
    "- Award1 = {\"agency\": \"US National Science Foundation\", \"award_code\": \"1440133\", \"award_URL\": \"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1440133&HistoricalAwards=false\"}\n",
    "- Award2 = {\"agency\": \"US National Science Foundation\", \"award_code\": \"1639750\", \"award_URL\": \"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1639750&HistoricalAwards=false\"}\n",
    "- Award3 = {\"agency\": \"US National Science Foundation\", \"award_code\": \"1639554\", \"award_URL\": \"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1639554&HistoricalAwards=false\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "keywords=[\"TZVOLCANO\", \"CHORDS\", \"UNAVCO\", \"Artificial Intelligence\", \"Machine Learning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "Dye, Mike, D. Sarah Stamps, Myles Mason (2021), Jupyter Notebook: Toward autonomous detection of anomalous GNSS data via applied unsupervised artificial intelligence, EarthCube Annual Meeting 2021 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work In Progress - improvements\n",
    "\n",
    "- Update Results section\n",
    "\n",
    "- change references to RNN in neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested next steps\n",
    "- A Support Vector Machine should be investigated as a possible filtering mechanism.\n",
    "- CHORDS API should be made more robust and flexible.\n",
    "- Predictions from the improved trained neural net model should be compared in real-time to incoming GNSS data to attempt to identify emerging volcanic events.\n",
    "- It would be interesting to see if filtering data with *both* the Gaussian Mixtures and K-means in combination would further improve the neural net predictions.\n",
    "- Use this same filtering process on time-series data from other CHORDS portals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements \n",
    "\n",
    "- CHORDS:\n",
    "for providing a versatile and practical cyber-infrastructure component\n",
    "- Virginia Tech:\n",
    "for enabling an incredibly supportive cutting edge learning and research environment\n",
    "- EarthCube & Earthcube Office: \n",
    "for creating the opportunity to create and share notebook, and creating a well-designed Jupyter notebook template \n",
    "- Abbi Devins-Suresh:\n",
    "for testing this notebook and invaluable feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "This notebook is licensed under the [MIT License](License.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "A brief definition of these terms is provided below for terms that may be unfamiliar to those without experience with machine learning, or are used in ways that may be unusual or ambiguous.\n",
    "\n",
    "- **[Feature](https://en.wikipedia.org/wiki/Feature_(machine_learning)**: \n",
    "\"a individual property or characteristic of a phenomenon being observed.\"<sup>[2]</sup> In this notebook, the imported fields (Time, Height, Longitude, and Latitude) are the initial features. One additional feature is calculated on the fly - the vector magnitude of scaled values of the original fields.\n",
    "\n",
    "- **Impute**: \n",
    "In machine learning, the replacement of null or missing values with an actual value in order to facilitate processing by an algorithm.\n",
    "\n",
    "- **Anomaly**: \n",
    "Data that for varying reasons do not occur within the usual ranges. In this notebook, there are (at least) two types of anomalies that may occur: those due to inaccurate measurements and subsequent processing, and those due to actual volcanic events. \n",
    "    \n",
    "Reference: [Wikipedia](https://en.wikipedia.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System functionality\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting and Vizualizations\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pipeline, scaling, normalizing, etc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# MSE calculation\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Algorithms\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-454c52984928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# TensorFlow ≥2.0 is required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m\"2.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Neural Network Support\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "    \n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "    \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Function definitions are included in this file to improve code readability \n",
    "# \n",
    "# Many of these functions are based on code from the excellent book\n",
    "# Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\n",
    "# by Aurélien Géron\n",
    "#####\n",
    "\n",
    "# Include local library paths\n",
    "sys.path.append('./libraries')\n",
    "\n",
    "# Import local libraries\n",
    "\n",
    "# Misc. functions to support data analysis\n",
    "from TZVOLCANO_utilities import *\n",
    "from TZVOLCANO_plotting import *\n",
    "from TZVOLCANO_gaussian_mixtures import *\n",
    "from TZVOLCANO_kmeans import *\n",
    "from TZVOLCANO_neural_net import *\n",
    "\n",
    "# CHORDS GUI interface (uses ChordsAPI.py)\n",
    "from chords_gui import chords_gui "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CHORDS GUI and load the API. DOMAIN is the URL of your CHORDS portal. \n",
    "DOMAIN = 'tzvolcano.chordsrt.com'\n",
    "CHORDS = chords_gui(DOMAIN)\n",
    "\n",
    "# Define the initial start and end date for the date selector\n",
    "INITIAL_START_DATE = '2021-01-01'\n",
    "INITIAL_END_DATE = '2021-01-01'\n",
    "\n",
    "\n",
    "# Min/max values used to scale the height, lon, and lat\n",
    "# This should be either -1, 1 or 0, 1\n",
    "SCALE_MINIMUM = 0\n",
    "SCALE_MAXIMUM = 1\n",
    "\n",
    "#####\n",
    "# Define important parameters that control the neural net functionality.\n",
    "#####\n",
    "N_STEPS_TRAINING = 6 * 1000      # The number of data points to use in the training set\n",
    "N_STEPS_FORECAST = 500           # The number of data points to display in the predictions graph\n",
    "N_STEPS_AHEAD = 100              # the number of steps ahead that the neural net will predict\n",
    "\n",
    "\n",
    "# Gaussian Mixtures Parameters\n",
    "N_COMPONENTS = 1                 # The number of regions to generate - needs to be 1 for this use case\n",
    "N_INIT = 10                      \n",
    "COVARIANCE_TYPE = \"tied\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "\n",
    "# Data import\n",
    "\n",
    "Data for this notebook is stored in the [TZVOLCANO CHORDS portal](http://tzvolcano.chordsrt.com/). In order to run the notebook, users will need to select a date range, an instrument identifier, and click the \"Download File\" button. The designated data file is then downloaded to the local server and can be selected for use.\n",
    "\n",
    "The data is is CSV format, and is downloaded using the CHORDS API.\n",
    "\n",
    "Each row of data includes a time, latitude, longitude, and height of instruments deployed on the Ol Doinyo Lengai volcano for the TZVOLCANO initiative. Below is a map of the instrument IDs to the TZVOLCANO designators:\n",
    "```\n",
    "Instrument ID    Instrument Name    Notes\n",
    "1                OLO1               Live instrument\n",
    "2                OLO3               Live instrument\n",
    "4                OLOT               Test instrument with data from station BILL_RTX\n",
    "5                OLO6               Live instrument\n",
    "6                OLO7               Live instrument\n",
    "7                OLO8               Live instrument\n",
    "8                OLOJ               Test instrument \n",
    "9                OLON               Test instrument\n",
    "```\n",
    "\n",
    "The default for the notebook is to use OLO1 (instrument id 1). OLOT (instrument id 4) is a test site that contains data from station BILL_RTX. Data from other instruments may not be available for the default time window.\n",
    "\n",
    "The default start and end dates are both set to January 1, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note on imported data\n",
    "\n",
    "The entire date range is analyzed using the K-Means and the Gaussian Mixtures clustering identification algorithms. \n",
    "\n",
    "The large number of data points (millions of points) would, however, slow the training of the neural networks to an unusable pace when running on a server without GPU support (such as the servers used by mybinder). \n",
    "\n",
    "The **N_STEPS_TRAINING** parameter limits the number of data points fed to the neural network. 5,000 - 10,000 points seems to provide enough training data to make passable predictions and is still a small enough data set that training the net takes a reasonable amount of time (several minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the instrument ID and the start and end date of the data to be processed and analyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; color: red; font-weight: bold; font-size: 14pt;\"> The cell below must be run in order to display the CHORDS GUI! </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHORDS.start_end_widgets(INITIAL_START_DATE, INITIAL_END_DATE)\n",
    "\n",
    "# Make sure that at least the default data file has been downloaded\n",
    "number_of_data_files_found = len(CHORDS.get_availiable_files())\n",
    "if (number_of_data_files_found < 1):\n",
    "    CHORDS.download_csv_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data file for use during analysis\n",
    "\n",
    "Choose which of the downloaded CSV files ot use for analysis. \n",
    "\n",
    "These files are retained on the server running the notebook indefinitely. \n",
    "\n",
    "**Note that on shared servers (such as mybinder.org) very large files may not work properly if they consume all available disk space and or memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; color: red; font-weight: bold; font-size: 14pt;\"> The cell below must be run in order to display the data file selection widget! </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHORDS.select_data_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read contents of the selected file in to a pandas object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (CHORDS.available_data_files.value == None):\n",
    "    print(\"no files were found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = CHORDS.available_data_files.value\n",
    "print(\"Imported csv data from\" + file_name)\n",
    "\n",
    "unmodified_data = CHORDS.load_data_from_file(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Resample the data to fill in holes in the time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a copy of the unmodified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing points in the time series\n",
    "resampled_data = unmodified_data.copy()\n",
    "\n",
    "# Set the 'Time' field to be used as the index\n",
    "resampled_data = resampled_data.set_index('Time').sort_index()\n",
    "\n",
    "# Re-insert the 'Time' field, as changing it to be the index removes is as a referenceable field in the pandas object\n",
    "resampled_data['Time'] = resampled_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative method: Make copy of unmodified data **and** resample the time series to fill in any missing time values in the series\n",
    "The standard approach for anomaly detection recommends imputing missing data points in a time series. However, in this case the imputing approach was found to reduce the final accuracy of the predictions made by the neural network. The code below is commented out and is not executed. It was retained as documentation in case the imputing approach is helpful for other applications of this overall methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the unmodified data\n",
    "# resampled_data = unmodified_data.copy()\n",
    "\n",
    "# Fill in missing points in the time series, inserting any times missing from the series using NaN as the value\n",
    "# resampled_data = resampled_data.set_index('Time').sort_index().resample('1000ms').ffill()\n",
    "\n",
    "# Re-insert the 'Time' field, as the resampling process changed it to be the index\n",
    "# resampled_data['Time'] = resampled_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale the Features\n",
    "Many clustering algorithms are highly sensitive to min-max ranges - including both the K-means and Gaussian mixtures algorithms. Here a best practice is followed, and before running the clustering algorithms, the data are rescaled.\n",
    "\n",
    "Details of the scaling procedure can be found by examining the **scale_np_data** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale Height, Longitude and Latitude to the range between SCALE_MINIMUM and SCALE_MAXIMUM\n",
    "scaled_data = pd.DataFrame()\n",
    "\n",
    "# Convert the Time variable to Seconds Since Epoch\n",
    "scaled_data[\"Seconds Since Epoch\"] = resampled_data['Time'].astype(np.int64)\n",
    "# scaled_data[\"Time\"] = resampled_data['Time']\n",
    "\n",
    "scaled_data[\"Scaled Height\"] = scale_np_data(resampled_data[\"Height\"].to_numpy(), SCALE_MINIMUM, SCALE_MAXIMUM)\n",
    "scaled_data[\"Scaled Latitude\"] = scale_np_data(resampled_data[\"Latitude\"].to_numpy(), SCALE_MINIMUM, SCALE_MAXIMUM)\n",
    "scaled_data[\"Scaled Longitude\"] = scale_np_data(resampled_data[\"Longitude\"].to_numpy(), SCALE_MINIMUM, SCALE_MAXIMUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Vector Magnitude\n",
    "Treating the individual fields as a vector, calculate the vector magnitude value as a derived feature.\n",
    "\n",
    "Creating an derived feature (variable) is a common technique in machine learning. It often makes it possible to more easily detect patterns in correlated features. In this case, it makes it easier to identify regions of localized high-noise areas within the timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_list = ['Scaled Height', 'Scaled Latitude', 'Scaled Longitude']\n",
    "\n",
    "scaled_data[\"Vector Magnitude\"] = calculate_vector_magnitude(scaled_data, fields_list, SCALE_MINIMUM, SCALE_MAXIMUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Features\n",
    "Note the the \"Vector Magnitude\" feature in green. It's wide range of values makes it easier for the algorithms to identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "ax = plt.gca() # get current axis\n",
    "alpha = 0.6\n",
    "\n",
    "scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Scaled Height', color='blue',ax=ax, alpha=alpha)\n",
    "scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Scaled Latitude', color='red', ax=ax, alpha=alpha)\n",
    "scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Scaled Longitude', color='orange', ax=ax, alpha=alpha)\n",
    "scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Vector Magnitude', color='green', ax=ax, alpha=alpha)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the plot above Vector Magnitude feature in green. The wide range of values should make it easier for the clustering algorithms to identify areas of high noise within the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify outliers for each feature with the Gaussian Mixtures algorithm\n",
    "\n",
    "In order to save space and make the code more readable, several steps in this process have been abstracted out to functions in an external library.\n",
    "\n",
    "For each feature, the same processing and analysis is applied:\n",
    " - Designate which feature the is being used\n",
    " - Designate a **density threshold percent** that determines the percentage of data point that will be classified as being outliers\n",
    " - Perform transformations which scale and impute the data (see note) for the designated feature\n",
    " - Train a Gaussian Mixtures model with the imputed data\n",
    " - Generate a plot showing which points are flagged for removal\n",
    " \n",
    " A manual part of this process is performed at this point: Determining what value of the density threshold percent yields the best results. The values that appear in this code were arrived at by executing this process, looking at the generated plot, adjusting the  density threshold percent, and the re-running this section of the code. This was performed until the plot indicates that outliers are identified, and that points that should be retained are not flagged for removal. \n",
    " \n",
    "Note: Imputing the data fills in missing values with the most frequently occurring value for that variable. This is necessary as the algorithm cannot function with missing values for times in the series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers for the Scaled Height feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'Scaled Height'\n",
    "scaled_height_density_threshold_percent = 5.2\n",
    "\n",
    "scaled_height_data_imputed = transform_data_for_gaussian_mixtures(scaled_data, field_name)\n",
    "\n",
    "scaled_height_gm, scaled_height_anomalies = get_anomalies_using_gaussian_mixtures(scaled_height_data_imputed, scaled_height_density_threshold_percent)\n",
    "\n",
    "plot_gaussian_mixture_anomalies(scaled_height_gm, scaled_height_data_imputed, scaled_height_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red highlights in the plot above indicate the data points identified as being outside of the assigned density threshold. These are the points the will be removed in the data filtering and cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers for the Scaled Latitude feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'Scaled Latitude'\n",
    "scaled_latitude_density_threshold_percent = 3\n",
    "\n",
    "scaled_latitude_data_imputed = transform_data_for_gaussian_mixtures(scaled_data, field_name)\n",
    "\n",
    "scaled_latitude_gm, scaled_latitude_anomalies = get_anomalies_using_gaussian_mixtures(scaled_latitude_data_imputed, scaled_latitude_density_threshold_percent)\n",
    "\n",
    "plot_gaussian_mixture_anomalies(scaled_latitude_gm, scaled_latitude_data_imputed, scaled_latitude_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers for the Scaled Longitude feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'Scaled Longitude'\n",
    "scaled_longitude_density_threshold_percent = 7\n",
    "\n",
    "scaled_longitude_data_imputed = transform_data_for_gaussian_mixtures(scaled_data, field_name)\n",
    "\n",
    "scaled_longitude_gm, scaled_longitude_anomalies = get_anomalies_using_gaussian_mixtures(scaled_longitude_data_imputed, scaled_longitude_density_threshold_percent)\n",
    "\n",
    "plot_gaussian_mixture_anomalies(scaled_longitude_gm, scaled_longitude_data_imputed, scaled_longitude_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers for the Vector Magnitude feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'Vector Magnitude'\n",
    "vector_magnitude_density_threshold_percent = 7\n",
    "\n",
    "vector_magnitude_data_imputed = transform_data_for_gaussian_mixtures(scaled_data, field_name)\n",
    "\n",
    "vector_magnitude_gm, vector_magnitude_anomalies = get_anomalies_using_gaussian_mixtures(vector_magnitude_data_imputed, vector_magnitude_density_threshold_percent)\n",
    "\n",
    "plot_gaussian_mixture_anomalies(vector_magnitude_gm, vector_magnitude_data_imputed, vector_magnitude_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate the arrays of times to remove in to one object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list of anomalies to datetime data type so they can be used to filter the scaled data set\n",
    "scaled_height_times_to_remove    = pd.to_datetime(scaled_height_anomalies[:, 0], unit='ns', utc=True)\n",
    "scaled_longitude_times_to_remove = pd.to_datetime(scaled_longitude_anomalies[:, 0], unit='ns', utc=True)\n",
    "scaled_latitude_times_to_remove  = pd.to_datetime(scaled_latitude_anomalies[:, 0], unit='ns', utc=True)\n",
    "vector_magnitude_times_to_remove = pd.to_datetime(vector_magnitude_anomalies[:, 0], unit='ns', utc=True)\n",
    "\n",
    "\n",
    "# Consolidate the arrays of times to remove in to one object\n",
    "times_to_remove = scaled_height_times_to_remove\n",
    "times_to_remove = times_to_remove.union(scaled_longitude_times_to_remove)\n",
    "times_to_remove = times_to_remove.union(scaled_latitude_times_to_remove)\n",
    "times_to_remove = times_to_remove.union(vector_magnitude_times_to_remove)\n",
    "times_to_remove = times_to_remove.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new pandas object with the points identified by the algorithm removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the flagged times from the scaled data set \n",
    "g_m_cleaned_data = scaled_data.copy()\n",
    "g_m_cleaned_data = g_m_cleaned_data.drop(times_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the \"cleansed\" data on top of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# get current axis\n",
    "ax = plt.gca()\n",
    "alpha = 0.99\n",
    "\n",
    "\n",
    "scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Vector Magnitude', color='blue',ax=ax, alpha=alpha)\n",
    "g_m_cleaned_data.plot(kind='line',x='Seconds Since Epoch',y='Vector Magnitude', color='red',ax=ax, alpha=alpha)\n",
    "ax.legend([\"Vector Magnitude (Unfiltered) \", \"Vector Magnitude (Filtered using Gaussian Mixtures)\"]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above show that the value ranges of the filtered data is much less than seen in the unmodified data. This is a good indicator that training a machine learning algorithm (such as a neural net) should result in trained algorithm that produces more accurate predictions than were it to be trained using the unfiltered data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify anomalies for each feature using the K-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find outliers for the Scaled Height feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_to_analyze = 'Scaled Height'\n",
    "number_of_clusters = 10\n",
    "\n",
    "# Impute the data \n",
    "scaled_height_kmeans_data_imputed = transform_data_for_kmeans(scaled_data, field_to_analyze)\n",
    "\n",
    "# Train the K-means model\n",
    "scaled_height_kmeans = KMeans(n_clusters=number_of_clusters, random_state=42)\n",
    "\n",
    "scaled_height_cluster_labels = scaled_height_kmeans.fit_predict(scaled_height_kmeans_data_imputed)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_decision_boundaries(scaled_height_kmeans, scaled_height_kmeans_data_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the regions generated by the K-means algorithm. The numbers indicate the index of the associated region.\n",
    "\n",
    "Some of the regioins for this particular K-means plot is a bit messy, yet it can still be used to identify regions in the data that likely contain high noise and should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the labeled regions from the decision boundary plot, decide which regions should be retained\n",
    "scaled_height_regions_to_retain = [4,1,5,3,0,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find outliers for the Scaled Latitude feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a K-means cluster \n",
    "field_to_analyze = 'Scaled Latitude'\n",
    "number_of_clusters = 5\n",
    "\n",
    "# Impute and scale the data \n",
    "scaled_latitude_kmeans_imputed_data = transform_data_for_kmeans(scaled_data, field_to_analyze)\n",
    "\n",
    "# Train the K-means model\n",
    "scaled_latitude_kmeans = KMeans(n_clusters=number_of_clusters, random_state=42)\n",
    "\n",
    "scaled_latitude_cluster_labels = scaled_latitude_kmeans.fit_predict(scaled_latitude_kmeans_imputed_data)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_decision_boundaries(scaled_latitude_kmeans, scaled_latitude_kmeans_imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the labeled regions from the decision boundary plot, decide which regions should be retained\n",
    "scaled_latitude_regions_to_retain = [1,3,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find outliers for the Scaled Longitude feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_to_analyze = 'Scaled Longitude'\n",
    "number_of_clusters = 5\n",
    "\n",
    "\n",
    "# Impute the data \n",
    "scaled_longitude_kmeans_data_imputed = transform_data_for_kmeans(scaled_data, field_to_analyze)\n",
    "\n",
    "# Train the K-means model\n",
    "scaled_longitude_kmeans = KMeans(n_clusters=number_of_clusters, random_state=41)\n",
    "\n",
    "scaled_longitude_cluster_labels = scaled_longitude_kmeans.fit_predict(scaled_longitude_kmeans_data_imputed)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_decision_boundaries(scaled_longitude_kmeans, scaled_longitude_kmeans_data_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the labeled regions from the decision boundary plot, decide which regions should be retained\n",
    "scaled_longitude_regions_to_retain = [3,0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find outliers for the Vector Magnitude feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_to_analyze = 'Vector Magnitude'\n",
    "number_of_clusters = 10\n",
    "\n",
    "# Impute the data \n",
    "vector_magnitude_kmeans_data_imputed = transform_data_for_kmeans(scaled_data, field_to_analyze)\n",
    "\n",
    "# Train the K-means model\n",
    "vector_magnitude_kmeans = KMeans(n_clusters=number_of_clusters, random_state=42)\n",
    "\n",
    "vector_magnitude_cluster_labels = vector_magnitude_kmeans.fit_predict(vector_magnitude_kmeans_data_imputed)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_decision_boundaries(vector_magnitude_kmeans, vector_magnitude_kmeans_data_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means did a nice job of locating the lower-noise signal for the derived Vector Magnitude feature - significantly better than for any of the individual un-combined features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the labeled regions from the decision boundary plot, decide which regions should be retained\n",
    "vector_magnitude_regions_to_retain = [4,1,6,3,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectively remove data points from the list\n",
    "\n",
    "# Create a new containerd for the cleaned data\n",
    "# cleaned_kmeans_data = pd.DataFrame()\n",
    "\n",
    "# Convert the Time variable to Seconds Since Epoch\n",
    "# cleaned_kmeans_data[\"Seconds Since Epoch\"] = resampled_data['Time'].astype(np.int64)\n",
    "\n",
    "cleaned_kmeans_data = scaled_data\n",
    "\n",
    "\n",
    "# cleaned_kmeans_data[\"Height\"] = scaled_data[\"Height\"]\n",
    "# cleaned_kmeans_data[\"Longitude\"] = scaled_data[\"Longitude\"]\n",
    "# cleaned_kmeans_data[\"Latitude\"] = scaled_data[\"Latitude\"]\n",
    "# cleaned_kmeans_data[\"Vector Magnitude\"] = scaled_data[\"Vector Magnitude\"]\n",
    "\n",
    "\n",
    "# Create a new column in the panadas object for the cluster labels for each dimension\n",
    "cleaned_kmeans_data['scaled_height_kmeans'] = scaled_height_cluster_labels\n",
    "cleaned_kmeans_data['scaled_longitude_kmeans'] = scaled_longitude_cluster_labels\n",
    "cleaned_kmeans_data['scaled_latitude_kmeans'] = scaled_latitude_cluster_labels\n",
    "cleaned_kmeans_data['vector_magnitude_kmeans'] = vector_magnitude_cluster_labels\n",
    "\n",
    "\n",
    "# For each dimension, keep only the designated rows\n",
    "cleaned_kmeans_data = cleaned_kmeans_data[cleaned_kmeans_data.scaled_height_kmeans.isin(scaled_height_regions_to_retain)]\n",
    "cleaned_kmeans_data = cleaned_kmeans_data[cleaned_kmeans_data.scaled_longitude_kmeans.isin(scaled_longitude_regions_to_retain)]\n",
    "cleaned_kmeans_data = cleaned_kmeans_data[cleaned_kmeans_data.scaled_latitude_kmeans.isin(scaled_latitude_regions_to_retain)]\n",
    "cleaned_kmeans_data = cleaned_kmeans_data[cleaned_kmeans_data.vector_magnitude_kmeans.isin(vector_magnitude_regions_to_retain)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on unfiltered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data in the proper format to train the RNN\n",
    "\n",
    "RNN_training_data, RNN_training_labels = get_RNN_training_sets(scaled_data, 'Scaled Height', N_STEPS_TRAINING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and train the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_neural_net_model(N_STEPS_AHEAD)\n",
    "\n",
    "history = model.fit(RNN_training_data, RNN_training_labels, epochs=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Take a look at the training set\n",
    "# print(RNN_training_labels.shape)\n",
    "# print(RNN_training_labels.shape)\n",
    "# plot_series(RNN_training_labels[0, :, 0], N_STEPS_TRAINING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the trained model to make prediction for *N_STEPS_AHEAD* time increments \n",
    "Only use the number of points for the predictions that were used for training the neural net (plus the number of steps ahead that we want to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_records = N_STEPS_TRAINING + N_STEPS_AHEAD\n",
    "truncated_scaled_data = scaled_data.head(number_of_records)\n",
    "\n",
    "# truncated_scaled_data.plot(kind='line',x='Seconds Since Epoch',y='Scaled Height', color='blue')\n",
    "\n",
    "X_new, Y_new = get_RNN_forecast_sets(truncated_scaled_data, 'Scaled Height', N_STEPS_TRAINING, N_STEPS_FORECAST, N_STEPS_AHEAD )\n",
    "\n",
    "X = X_new\n",
    "\n",
    "Y_pred = model.predict(X_new)[:, -1][..., np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the entire training set \n",
    "# plot_series(RNN_training_labels[0, :, 0], N_STEPS_TRAINING)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot the last n_step_forecast\n",
    "# plot_multiple_forecasts(X_new, Y_new, Y_pred, N_STEPS_FORECAST)\n",
    "# # save_fig(\"forecast_ahead_plot\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on data filtered using Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_RNN_training_data, cleaned_RNN_training_labels = get_RNN_training_sets(g_m_cleaned_data, 'Scaled Height', N_STEPS_TRAINING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and train the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new set of RNN training data based on the cleansed data\n",
    "cleaned_model = get_neural_net_model(N_STEPS_AHEAD)\n",
    "\n",
    "cleaned_history = cleaned_model.fit(cleaned_RNN_training_data, cleaned_RNN_training_labels, epochs=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plot_series(cleaned_RNN_training_data[0, :, 0], N_STEPS_TRAINING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the cleaned/filterted data. Note that The large spikes have been completely removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the trained model to make prediction for *N_STEPS_AHEAD* time increments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_records = N_STEPS_TRAINING + N_STEPS_AHEAD\n",
    "truncated_g_m_cleaned_data = g_m_cleaned_data.head(number_of_records)\n",
    "\n",
    "X_cleaned_new, Y_cleaned_new = get_RNN_forecast_sets(truncated_g_m_cleaned_data, 'Scaled Height', N_STEPS_TRAINING-1, N_STEPS_FORECAST, N_STEPS_AHEAD)\n",
    "\n",
    "X_cleaned = X_cleaned_new\n",
    "\n",
    "Y_cleaned_pred = cleaned_model.predict(X_cleaned_new)[:, -1][..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on data filtered using K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cleaned_RNN_training_data, kmeans_cleaned_RNN_training_labels = get_RNN_training_sets(cleaned_kmeans_data, 'Scaled Height', N_STEPS_TRAINING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #### Create and train the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new set of RNN training data based on the cleansed data\n",
    "kmeans_cleaned_model = get_neural_net_model(N_STEPS_AHEAD)\n",
    "\n",
    "kmeans_cleaned_history = kmeans_cleaned_model.fit(kmeans_cleaned_RNN_training_data, kmeans_cleaned_RNN_training_labels, epochs=20,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the trained model to make prediction for *N_STEPS_AHEAD* time increments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_records = N_STEPS_TRAINING + N_STEPS_AHEAD\n",
    "truncated_kmeans_cleaned_data = cleaned_kmeans_data.head(number_of_records)\n",
    "\n",
    "X_kmeans_cleaned_new, Y_kmeans_cleaned_new = get_RNN_forecast_sets(truncated_kmeans_cleaned_data, 'Scaled Height', N_STEPS_TRAINING-1, N_STEPS_FORECAST, N_STEPS_AHEAD)\n",
    "\n",
    "X_kmeans_cleaned = X_kmeans_cleaned_new\n",
    "\n",
    "Y_kmeans_cleaned_pred = kmeans_cleaned_model.predict(X_kmeans_cleaned_new)[:, -1][..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of unfiltered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the entire training set \n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_series(RNN_training_data[0, :, 0], N_STEPS_TRAINING)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is the plot for the unfiltered data. Note the large spikes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the last n_step_forecast\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_multiple_forecasts(X_new, Y_new, Y_pred, N_STEPS_FORECAST)\n",
    "# save_fig(\"forecast_ahead_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot compares how well the model did at predicting future time series points. The actual values from the time series are shown in red underneath the dark blue Xs indicating predictions made by the neural net trained on the unfiltered (uncleaned) data. Note that the forecast points aren't particularly tightly clustered around the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of Gaussian Mixtures cleaned / filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the entire training set for comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_series(cleaned_RNN_training_data[0, :, 0], N_STEPS_TRAINING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of the data cleaned by the Gaussian Mixtures algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plot_multiple_forecasts(X_cleaned_new, Y_cleaned_new, Y_cleaned_pred, N_STEPS_FORECAST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison plot for the predictions made by neural network trained on the data filtered by the Gaussian Mixtures algorithm. The forecast points are much closer to the actual values than in the corresponding plot for the unfiltered data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of K-means cleaned / filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the entire training set for comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_series(kmeans_cleaned_RNN_training_data[0, :, 0], N_STEPS_TRAINING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of the data cleaned by the K-means algorithm. Note that visually it appears to be even more smooth than the plot of the data filtered using Gaussian Mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plot_multiple_forecasts(X_kmeans_cleaned_new, Y_kmeans_cleaned_new, Y_kmeans_cleaned_pred, N_STEPS_FORECAST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison plot for the predictions made by neural network trained on the data filtered by the K-mean algorithm. The forecast points are again closer to the actual values... but perhaps not quite as close as seen for the Gaussian Mixtures filtered data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the accuracy of the prediction made by the the different trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MSE of the unmodified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(Y_new.flatten(), Y_pred.flatten())\n",
    "mse\n",
    "\n",
    "print(\"Mean squared error for the unmodified data: \" + str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MSE of the Gaussian Mixtures cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_m_cleaned_mse = mean_squared_error(Y_cleaned_new.flatten(), Y_cleaned_pred.flatten())\n",
    "\n",
    "print(\"Mean squared error for the data cleaned using Gaussian Mixtures: \" + str(g_m_cleaned_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MSE of the K-Means cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cleaned_mse = mean_squared_error(Y_kmeans_cleaned_new.flatten(), Y_kmeans_cleaned_pred.flatten())\n",
    "\n",
    "print(\"Mean squared error for the data cleaned using K-Means: \" + str(kmeans_cleaned_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percent improvement in neural network predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_m_diff = (g_m_cleaned_mse/mse)*100\n",
    "kmeans_diff = (kmeans_cleaned_mse/mse)*100\n",
    "\n",
    "print(\"\\n\\nFor this run, the removal of the identified data points  results in a \")\n",
    "print(str(round(100-g_m_diff)) + \"% (Gaussian Mixtures)\")\n",
    "print(str(round(100-kmeans_diff)) + \"% (K-means)\")\n",
    "print(\"reduced error rate in predicting values using the trained neural net\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the Gaussian Mixtures and the K-means approach seem to significantly improve the prediction accuracy of the neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "Daniels, M. D., Kerkez, B., Chandrasekar, V., Graves, S., Stamps, D. S., Martin, C., Dye, M., Gooch, R., Bartos, M., Jones, J., Keiser, K., 2016, Cloud-Hosted Real-time Data Services for the Geosciences (CHORDS) software (Version 0.9). UCAR/NCAR - Earth Observing Laboratory. https://doi.org/10.5065/d6v1236q\n",
    "\n",
    "Kerkez, B., Daniels, M., Graves, S., Chandrasekar, V., Keiser, K., Martin, C., … Vernon, F. (2016). Cloud Hosted Real‐time Data Services for the Geosciences (CHORDS). Geoscience Data Journal, 3(1), 4–8. doi:10.1002/gdj3.36\n",
    "\n",
    "Stamps, D. S., Saria, E., Ji, K. H., Jones, J. R., Ntambila, D., Daniels, M. D., & Mencin, D. (2016). Real-time data from the Tanzania Volcano Observatory at the Ol Doinyo Lengai volcano in Tanzania (TZVOLCANO). UCAR/NCAR - Earth Observing Laboratory. https://doi.org/10.5065/d6p849bm\n",
    "\n",
    "Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2nd ed.). O'Reilly.\n",
    "\n",
    "Wikimedia Foundation. (2021, February 3). Wikipedia. https://en.wikipedia.org/. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.182px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
