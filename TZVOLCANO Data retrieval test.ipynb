{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imported libraries are listed herefor readbility\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from zlib import crc32\n",
    "\n",
    "\n",
    "# Data pipeline, scaling, normalizing, etc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Transforming & Manipulating data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# The Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for data retrieval\n",
    "INSTRUMENT_ID = '1'\n",
    "START = \"2021-01-20T00:00\"\n",
    "END = \"2021-01-21T00:00\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_olo_data(instrument_id, start, end):\n",
    "    url = f'http://tzvolcano.chordsrt.com/api/v1/data/{instrument_id}.csv?start={start}&end={end}'\n",
    "    print(url)\n",
    "\n",
    "    \n",
    "    return pd.read_csv(url,\n",
    "#                     index_col='Time', \n",
    "                    parse_dates=['Time'],\n",
    "                    header=18\n",
    "                    )\n",
    "\n",
    "\n",
    "# original_data.info()\n",
    "# original_data.describe()\n",
    "# original_data.head()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = load_olo_data(INSTRUMENT_ID, START, END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the seconds_since_epoch variable\n",
    "original_data[\"seconds_since_epoch\"] = original_data['Time'].astype(np.int64)\n",
    "\n",
    "# Show all the keys (variable names) in the loaded data set\n",
    "original_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for the data\n",
    "original_data.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for Longitude & Latitude\n",
    "# Scatter plots are useful to identify patterns between the variables \n",
    "# and to see which ones will be useful for machine learning)\n",
    "original_data.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", alpha=0.1)\n",
    "original_data.plot(kind=\"scatter\", x=\"Height\", y=\"Longitude\", alpha=0.1)\n",
    "original_data.plot(kind=\"scatter\", x=\"Height\", y=\"Latitude\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_data.plot(x=\"seconds_since_epoch\", y=\"Height\")\n",
    "original_data.plot(x=\"seconds_since_epoch\", y=\"Longitude\")\n",
    "original_data.plot(x=\"seconds_since_epoch\", y=\"Latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show correclations between the variables\n",
    "original_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an overview of info about the data\n",
    "original_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    #     Seed ramdom generator so the same indices are retrieved every time\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "# Split the original data in to two sets: one used for training and one used for testing the generated model\n",
    "#  .2  = 20% of the total data set will be used as test data\n",
    "train_data, test_data = split_train_test(original_data, 0.2)\n",
    "\n",
    "# Show how many entries are in the total, training and test sets (jsut for confirmation)\n",
    "print(len(original_data))\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \"Time\" variable from the data set, as it is not useful for the model\n",
    "numerical_data = original_data.drop([\"Time\"], axis=1)\n",
    "\n",
    "# Print out the remaing variable names, just to see we did remove \"Time\"\n",
    "numerical_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipline to clean numerical data\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Test that the pipeline works\n",
    "original_transformed = num_pipeline.fit_transform(numerical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definte the pipleine for ALL the data \n",
    "# (right now we only have numerical data, but this is not always the case)\n",
    "\n",
    "num_attributes = list(numerical_data)\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attributes)\n",
    "])\n",
    "\n",
    "# Prepare the full set of training data\n",
    "training_prepared = full_pipeline.fit_transform(numerical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Linear Regression Model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(training_prepared,training_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a few predictions\n",
    "some_data = numerical_data.iloc[:5]\n",
    "# some_labels = numerical_data.iloc[:5].drop('Latitude', axis=1).drop('Longitude', axis=1).drop('Height', axis=1)\n",
    "some_labels = numerical_data.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels\", list(some_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
